{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairLogReg(nn.Module):\n",
    "    def __init__(self, D, warm_start = None):\n",
    "        super(FairLogReg, self).__init__()\n",
    "        if warm_start is not None:\n",
    "            self.theta = torch.nn.Parameter(warm_start, requires_grad=True)\n",
    "        else:\n",
    "            self.theta = torch.nn.Parameter(torch.zeros(D), requires_grad=True)\n",
    "        self.old_theta = tensor(float(\"Inf\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mv(self.theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick between COMPAS and Adult data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "\n",
    "def get_adults_datset(n_samples, with_val):\n",
    "    col_names = [\n",
    "        \"age\",\n",
    "        \"workclass\",\n",
    "        \"fnlwgt\",\n",
    "        \"education\",\n",
    "        \"education-num\",\n",
    "        \"marital-status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"capital-gain\",\n",
    "        \"capital-loss\",\n",
    "        \"hours-per-week\",\n",
    "        \"native-country\",\n",
    "        \"income\",\n",
    "    ]\n",
    "\n",
    "    # Load data\n",
    "    data = (\n",
    "        pd.read_csv(\"/root/fairclassification/datasets/adult.data\", names=col_names)\n",
    "        .sample(frac=1.0, random_state=42)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Handle missing values\n",
    "    data = data.replace(\" ?\", pd.NA)\n",
    "    data = data.dropna()\n",
    "\n",
    "    if with_val:\n",
    "        data = data.iloc[:-5000].copy()\n",
    "\n",
    "    # Define sensitive attribute\n",
    "    sensitive_attr = tensor(data['sex'].apply(lambda x: not 'Female' in x).values)\n",
    "    # data[\"sex\"].apply(lambda x: \"Female\" in x).values\n",
    "\n",
    "    # Define target variable\n",
    "    y = LabelEncoder().fit_transform(data[\"income\"])\n",
    "\n",
    "    # Define features\n",
    "    X = data.drop([\"sex\", \"income\"], axis=1)\n",
    "\n",
    "    # Normalization for numeric features\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "    # One-hot encoding for categorical features\n",
    "    X = pd.get_dummies(X).values\n",
    "    X = torch.from_numpy(X.astype(\"float32\"))  # Convert to float32 for PyTorch\n",
    "    y = torch.from_numpy(y.astype(\"float32\"))\n",
    "    # sensitive_attr = torch.from_numpy(sensitive_attr.astype(\"long\"))\n",
    "    if not with_val:\n",
    "        X = X[-5000:, :]\n",
    "        y = y[-5000:]\n",
    "        sensitive_attr = sensitive_attr[-5000:]\n",
    "    return X[:n_samples, :], y[:n_samples], sensitive_attr[:n_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compas_dataset(n_samples, with_val):\n",
    "    col_names = [\n",
    "        \"sex\",\n",
    "        \"age\",\n",
    "        \"race\",\n",
    "        \"juv_fel_count\",\n",
    "        \"juv_misd_count\",\n",
    "        \"juv_other_count\",\n",
    "        \"priors_count\",\n",
    "        \"c_charge_degree\",\n",
    "        \"two_year_recid\",\n",
    "    ]\n",
    "\n",
    "    # Load data\n",
    "    data = (\n",
    "        pd.read_csv(\"/root/fairclassification/datasets/compas-scores-two-years.csv\")\n",
    "        .sample(frac=1.0, random_state=42)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    data = data.loc[:, col_names]\n",
    "\n",
    "    # Handle missing values\n",
    "    data = data.replace(\" ?\", pd.NA)\n",
    "    data = data.dropna()\n",
    "\n",
    "    if with_val:\n",
    "        data = data.iloc[:-2000].copy()\n",
    "    else:\n",
    "        data = data.iloc[-2000:].copy()\n",
    "\n",
    "    # Define sensitive attribute\n",
    "    sensitive_attr = tensor(data[\"race\"].apply(lambda x: \"African-American\" in x).values)\n",
    "\n",
    "    # Define target variable\n",
    "    y = LabelEncoder().fit_transform(data[\"two_year_recid\"])\n",
    "\n",
    "    # Define features\n",
    "    X = data.drop([\"race\", \"two_year_recid\"], axis=1)\n",
    "\n",
    "    # Normalization for numeric features\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "    # One-hot encoding for categorical features\n",
    "    X = pd.get_dummies(X).values\n",
    "    X = torch.from_numpy(X.astype(\"float32\"))  # Convert to float32 for PyTorch\n",
    "    y = torch.from_numpy(y.astype(\"float32\"))\n",
    "    return X[:n_samples, :], y[:n_samples], sensitive_attr[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment one out\n",
    "\n",
    "# def get_data(filename):\n",
    "#     df = pd.read_csv(\"data/COMPAS/\" + filename + \".csv\")\n",
    "#     s = tensor(df['race'] == \"Caucasian\")\n",
    "#     y = tensor(df['two_year_recid'] == 0).float()\n",
    "#     X = tensor(df.drop(columns=['race','sex','sex-race','two_year_recid']).values).float()\n",
    "#     X = torch.cat((torch.ones(X.shape[0],1), X), dim=1)\n",
    "#     return (X,y,s)\n",
    "\n",
    "# lam_regs = 2. ** np.array([-3, -3, -3, -3, -3])\n",
    "# lam_regs = 10. ** np.array([-3, -3, -3, -3, -3])\n",
    "# lam_regs = np.zeros_like(lam_regs)\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(\"data/Adult/\" + filename + \".csv\")\n",
    "    s = tensor(df['sex'] == \"Male\")\n",
    "    y = tensor(df['income-per-year'] == \">50K\").float()\n",
    "    X = tensor(df.drop(columns=['sex','race','income-per-year','race-sex','capital-gain', 'capital-loss']).values).float()\n",
    "    X = torch.cat((torch.ones(X.shape[0],1), X), dim=1)\n",
    "    return (X,y,s)\n",
    "lam_regs = 2. ** np.array([-14, -12, -12, -12, -13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "form=\"linear\"\n",
    "sum_form=1 # 1 for sum, -1 for difference\n",
    "unfairness = \"eop\"\n",
    "\n",
    "if unfairness == \"eoo\":\n",
    "    eoo=True\n",
    "    eo=False\n",
    "elif unfairness == \"dp\":\n",
    "    eoo=False\n",
    "    eo=False\n",
    "else:\n",
    "    eoo=False\n",
    "    eo=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if form == \"logistic\":\n",
    "    def g(outputs):\n",
    "        return -F.logsigmoid(-outputs).sum()\n",
    "elif form == \"hinge\":\n",
    "    relu = torch.nn.ReLU()\n",
    "    def g(outputs):\n",
    "        return relu(outputs+1).sum()\n",
    "elif form == \"linear\":\n",
    "    def g(outputs):\n",
    "        return outputs.sum()\n",
    "else:\n",
    "    raise ValueError(\"Pick a valid form!\")\n",
    "\n",
    "ploss = nn.BCEWithLogitsLoss()\n",
    "def floss(outputs, sens_attr, Pa, Pb):\n",
    "    return sum_form * g(sum_form * outputs[sens_attr])/Pa + g(- outputs[~sens_attr])/Pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'compas'\n",
    "\n",
    "if dataset == 'compas':\n",
    "    get_data = get_compas_dataset\n",
    "elif dataset == 'adult':\n",
    "    get_data = get_adults_datset\n",
    "\n",
    "(Xs, ys, ss) = ([None] * 1, [None] * 1, [None] * 1)\n",
    "(Xts, yts, sts) = ([None] * 1, [None] * 1, [None] * 1)\n",
    "for i in range(1):\n",
    "    (Xs[i], ys[i], ss[i]) = get_data(n_samples=50000, with_val=True)\n",
    "    (Xts[i], yts[i], sts[i]) = get_data(n_samples=1000, with_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Xs, ys, ss) = ([None] * 5, [None] * 5, [None] * 5)\n",
    "# (Xts, yts, sts) = ([None] * 5, [None] * 5, [None] * 5)\n",
    "# for i in range(5):\n",
    "#     (Xs[i], ys[i], ss[i]) = get_data(\"train\" + str(i))\n",
    "#     (Xts[i], yts[i], sts[i]) = get_data(\"test\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_closure(model, optimizer, lam_fair, lam_reg, X, y, s, Pa, Pb):\n",
    "    def closure():\n",
    "        assert not torch.isnan(model.theta).any()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        if eoo:\n",
    "            loss = ploss(outputs,y) + lam_reg * (model.theta**2).mean() + lam_fair/outputs.shape[0] * floss(outputs[y.bool()], s[y.bool()], Pa, Pb)\n",
    "        elif eo:\n",
    "            loss = ploss(outputs,y) + lam_reg * (model.theta**2).mean() + 0.5*lam_fair/outputs.shape[0] * (floss(outputs[y.bool()], s[y.bool()], Pa[0], Pb[0]) + floss(outputs[~y.bool()], s[~y.bool()], Pa[1], Pb[1]))\n",
    "        else:\n",
    "            loss = ploss(outputs,y) + lam_reg * (model.theta**2).mean() + lam_fair/outputs.shape[0] * floss(outputs, s, Pa, Pb)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    return closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,y,s,lam_fair=0, lam_reg=0, warm_start=None):\n",
    "    if eoo:\n",
    "        (Pa, Pb) = ((s & y.bool()).float().mean(), (~s&y.bool()).float().mean())\n",
    "    if eo:\n",
    "        Pa = [(s & y.bool()).float().mean(), (s & ~y.bool()).float().mean()]\n",
    "        Pb = [(~s & y.bool()).float().mean(), (~s & ~y.bool()).float().mean()]\n",
    "    else:\n",
    "        (Pa, Pb) = (s.float().mean(), 1 - s.float().mean())\n",
    "    model = FairLogReg(X.shape[1], warm_start=warm_start)\n",
    "    if form == \"hingexxx\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    else:\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), lr=0.1)\n",
    "    closure = make_closure(model, optimizer, lam_fair, lam_reg, X, y, s, Pa, Pb)\n",
    "    for t in trange(500):\n",
    "        loss = optimizer.step(closure)\n",
    "        if form == \"hingexxx\":\n",
    "            scheduler.step(loss)\n",
    "        diff = (model.old_theta - model.theta).abs().max()\n",
    "        if diff < 1e-10:\n",
    "            break\n",
    "        model.old_theta = model.theta.clone().detach()\n",
    "    return (model, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(model, X,y,s, lam_fair=0, lam_reg=0):\n",
    "    (Pa, Pb) = (s.float().mean(), 1 - s.float().mean())\n",
    "    outputs = model(X)\n",
    "    accuracy = (y == (outputs >= 0)).float().mean()\n",
    "    if eoo:\n",
    "        # unfairness = (outputs[y.bool() & s] >= 0).float().mean() - (outputs[y.bool() & ~s] >= 0).float().mean()\n",
    "        unfairness = (torch.sigmoid(outputs[y.bool() & s])).float().mean() - (torch.sigmoid(outputs[y.bool() & ~s])).float().mean()\n",
    "        relaxation = 1/outputs.shape[0] * floss(outputs[y.bool()], s[y.bool()], Pa, Pb)\n",
    "    elif eo:\n",
    "        unfairness_y1 = 0.5*torch.abs((torch.sigmoid(outputs[y.bool() & s])).float().mean() - (torch.sigmoid(outputs[y.bool() & ~s])).float().mean())\n",
    "        unfairness_y0 = 0.5*torch.abs((torch.sigmoid(outputs[~y.bool() & s])).float().mean() - (torch.sigmoid(outputs[~y.bool() & ~s])).float().mean())\n",
    "        # unfairness_y1 = 0.5*torch.abs((outputs[y.bool() & s]>0).float().mean() - (outputs[y.bool() & ~s]>0).float().mean())\n",
    "        # unfairness_y0 = 0.5*torch.abs((outputs[~y.bool() & s]>0).float().mean() - (outputs[~y.bool() & ~s]>0).float().mean())\n",
    "        unfairness = unfairness_y0 + unfairness_y1\n",
    "        relaxation = 1/outputs.shape[0] * floss(outputs[y.bool()], s[y.bool()], Pa, Pb)\n",
    "    else:\n",
    "        unfairness = (torch.sigmoid(outputs[s])).float().mean() - (torch.sigmoid(outputs[~s])).float().mean()\n",
    "        # unfairness = (outputs[s]>0).float().mean() - (outputs[~s]>0).float().mean()\n",
    "        relaxation = 1/outputs.shape[0] * floss(outputs, s, Pa, Pb)\n",
    "    loss = ploss(outputs,y)\n",
    "    return(accuracy, unfairness, loss, relaxation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for weighting baseline, if desired\n",
    "def get_weighed_loss(X,y,s):\n",
    "    wobs = y * 10 + s\n",
    "    wobs[wobs==0.] = (wobs==0.).float().mean()\n",
    "    wobs[wobs==1.] = (wobs==1.).float().mean()\n",
    "    wobs[wobs==11.] = (wobs==11.).float().mean()\n",
    "    wobs[wobs==10.] = (wobs==10.).float().mean()\n",
    "    wy = (y - (y==0).float().mean()).abs()\n",
    "    ws = (s.float() - (s==0).float().mean()).abs()\n",
    "    wexp = ws * wy\n",
    "    return nn.BCEWithLogitsLoss(weight = (wexp/wobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  ..., False,  True, False])\n"
     ]
    }
   ],
   "source": [
    "sc = ss[0]\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 17/500 [00:07<03:18,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.0], 'Accuracy': [0.6620000004768372], 'Unfairness': [0.10620817542076111], 'Ploss': [0.6226617097854614], 'Relaxation': [0.3119054436683655]}\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/500 [00:04<02:46,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.01], 'Accuracy': [0.6610000133514404], 'Unfairness': [0.10327711701393127], 'Ploss': [0.6221479773521423], 'Relaxation': [0.29832935333251953]}\n",
      "0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/500 [00:05<04:38,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.02], 'Accuracy': [0.6629999876022339], 'Unfairness': [0.10035611689090729], 'Ploss': [0.6218308210372925], 'Relaxation': [0.2852500081062317]}\n",
      "0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18/500 [00:07<03:28,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.03], 'Accuracy': [0.6660000085830688], 'Unfairness': [0.09745518863201141], 'Ploss': [0.6216985583305359], 'Relaxation': [0.2726800739765167]}\n",
      "0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 16/500 [00:05<02:49,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.04], 'Accuracy': [0.6669999957084656], 'Unfairness': [0.09457287192344666], 'Ploss': [0.6217386722564697], 'Relaxation': [0.26058170199394226]}\n",
      "0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 19/500 [00:06<02:34,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.05], 'Accuracy': [0.6690000295639038], 'Unfairness': [0.09170591831207275], 'Ploss': [0.6219398379325867], 'Relaxation': [0.248912513256073]}\n",
      "0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12/500 [00:04<03:02,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.06], 'Accuracy': [0.6679999828338623], 'Unfairness': [0.08885304629802704], 'Ploss': [0.622292160987854], 'Relaxation': [0.23764188587665558]}\n",
      "0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/500 [00:03<03:10,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.07], 'Accuracy': [0.6650000214576721], 'Unfairness': [0.08601255714893341], 'Ploss': [0.6227868795394897], 'Relaxation': [0.2267397940158844]}\n",
      "0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/500 [00:07<03:56,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.08], 'Accuracy': [0.6660000085830688], 'Unfairness': [0.08318302035331726], 'Ploss': [0.6234161257743835], 'Relaxation': [0.21617862582206726]}\n",
      "0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 17/500 [00:04<02:18,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.09], 'Accuracy': [0.6650000214576721], 'Unfairness': [0.0803629606962204], 'Ploss': [0.6241729855537415], 'Relaxation': [0.20593298971652985]}\n",
      "0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/500 [00:05<03:04,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.1], 'Accuracy': [0.6589999794960022], 'Unfairness': [0.07755109667778015], 'Ploss': [0.6250511407852173], 'Relaxation': [0.19597981870174408]}\n",
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/500 [00:04<02:43,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.11], 'Accuracy': [0.6589999794960022], 'Unfairness': [0.07474710047245026], 'Ploss': [0.6260449290275574], 'Relaxation': [0.1863003671169281]}\n",
      "0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/500 [00:03<02:22,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.12], 'Accuracy': [0.6579999923706055], 'Unfairness': [0.07194846868515015], 'Ploss': [0.6271499991416931], 'Relaxation': [0.17686964571475983]}\n",
      "0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/500 [00:06<04:54,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.13], 'Accuracy': [0.6570000052452087], 'Unfairness': [0.06915512681007385], 'Ploss': [0.6283618211746216], 'Relaxation': [0.16767215728759766]}\n",
      "0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 17/500 [00:06<02:56,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.14], 'Accuracy': [0.6589999794960022], 'Unfairness': [0.06636613607406616], 'Ploss': [0.6296765804290771], 'Relaxation': [0.15869084000587463]}\n",
      "0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/500 [00:04<02:39,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.15], 'Accuracy': [0.6549999713897705], 'Unfairness': [0.06358091533184052], 'Ploss': [0.6310910582542419], 'Relaxation': [0.14990994334220886]}\n",
      "0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18/500 [00:05<02:27,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.16], 'Accuracy': [0.6499999761581421], 'Unfairness': [0.0607987642288208], 'Ploss': [0.6326023936271667], 'Relaxation': [0.14131470024585724]}\n",
      "0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/500 [00:06<03:20,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.17], 'Accuracy': [0.6480000019073486], 'Unfairness': [0.05801911652088165], 'Ploss': [0.6342081427574158], 'Relaxation': [0.13289134204387665]}\n",
      "0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/500 [00:04<02:22,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.18], 'Accuracy': [0.640999972820282], 'Unfairness': [0.05524151027202606], 'Ploss': [0.6359061598777771], 'Relaxation': [0.1246267631649971]}\n",
      "0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/500 [00:06<05:10,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.19], 'Accuracy': [0.6420000195503235], 'Unfairness': [0.052465423941612244], 'Ploss': [0.6376948952674866], 'Relaxation': [0.11650880426168442]}\n",
      "0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/500 [00:05<03:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.2], 'Accuracy': [0.6359999775886536], 'Unfairness': [0.049690574407577515], 'Ploss': [0.63957279920578], 'Relaxation': [0.1085258200764656]}\n",
      "0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/500 [00:05<03:44,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.21], 'Accuracy': [0.6330000162124634], 'Unfairness': [0.04691731929779053], 'Ploss': [0.6415383815765381], 'Relaxation': [0.10066892206668854]}\n",
      "0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/500 [00:05<04:09,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.22], 'Accuracy': [0.6320000290870667], 'Unfairness': [0.04414376616477966], 'Ploss': [0.643592119216919], 'Relaxation': [0.09292314946651459]}\n",
      "0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/500 [00:05<03:24,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.23], 'Accuracy': [0.6290000081062317], 'Unfairness': [0.04137028753757477], 'Ploss': [0.6457329392433167], 'Relaxation': [0.08528048545122147]}\n",
      "0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18/500 [00:07<03:11,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.24], 'Accuracy': [0.6240000128746033], 'Unfairness': [0.038596734404563904], 'Ploss': [0.6479607820510864], 'Relaxation': [0.07773133367300034]}\n",
      "0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12/500 [00:04<02:58,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.25], 'Accuracy': [0.6200000047683716], 'Unfairness': [0.03582274913787842], 'Ploss': [0.6502758264541626], 'Relaxation': [0.07026619464159012]}\n",
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 16/500 [00:05<02:46,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.26], 'Accuracy': [0.6169999837875366], 'Unfairness': [0.033048197627067566], 'Ploss': [0.6526784896850586], 'Relaxation': [0.06287610530853271]}\n",
      "0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 22/500 [00:06<02:21,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.27], 'Accuracy': [0.6100000143051147], 'Unfairness': [0.030272722244262695], 'Ploss': [0.655169665813446], 'Relaxation': [0.05555199086666107]}\n",
      "0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/500 [00:04<02:44,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.28], 'Accuracy': [0.6039999723434448], 'Unfairness': [0.027496173977851868], 'Ploss': [0.6577503681182861], 'Relaxation': [0.048285190016031265]}\n",
      "0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 17/500 [00:06<03:07,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split': [0], 'Type': ['Test'], 'Lam_fair': [0.29], 'Accuracy': [0.597000002861023], 'Unfairness': [0.024718254804611206], 'Ploss': [0.6604219675064087], 'Relaxation': [0.041067127138376236]}\n",
      "0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame(columns = ['Split', 'Lam_fair', 'Type', 'Accuracy', 'Unfairness', 'Ploss', 'Relaxation'])\n",
    "warm_starts = [None] * 1\n",
    "lfs = np.arange(0.0, 0.3, 0.01) #0.02)\n",
    "df = pd.DataFrame()\n",
    "for lam_fair in lfs:\n",
    "    for i in range(1):\n",
    "        (model,t) = train_model(Xs[i],ys[i],ss[i], lam_fair = lam_fair, lam_reg = lam_regs[i], warm_start=warm_starts[i])\n",
    "     #    (model,t) = train_model(Xs[i],ys[i],ss[i], lam_fair = lam_fair, lam_reg = 0, warm_start=warm_starts[i])\n",
    "        warm_starts[i] = model.theta.clone().detach()\n",
    "        (train_accuracy, train_unfairness, train_loss, train_relax) = get_summary(model, Xs[i], ys[i], ss[i], lam_fair = lam_fair, lam_reg = lam_regs[i])\n",
    "        d = {\"Split\": [i],\n",
    "             \"Type\": [\"Train\"],\n",
    "             \"Lam_fair\": [lam_fair.item()],\n",
    "             'Accuracy': [train_accuracy.item()], \n",
    "             'Unfairness': [train_unfairness.item()],\n",
    "             'Ploss': [train_loss.item()],\n",
    "             'Relaxation': [train_relax.item()]}\n",
    "        df = pd.concat([df, pd.DataFrame(d)], axis=0, ignore_index=True)\n",
    "        (test_accuracy, test_unfairness, test_loss, test_relax) = get_summary(model, Xts[i], yts[i], sts[i], lam_fair = lam_fair, lam_reg = lam_regs[i])\n",
    "        d = {\"Split\": [i],\n",
    "             \"Type\": [\"Test\"],\n",
    "             \"Lam_fair\": [lam_fair.item()],\n",
    "             'Accuracy': [test_accuracy.item()], \n",
    "             'Unfairness': [test_unfairness.item()],\n",
    "             'Ploss': [test_loss.item()],\n",
    "             'Relaxation': [test_relax.item()]}\n",
    "        print(d)\n",
    "        df = pd.concat([df, pd.DataFrame(d)], axis=0, ignore_index=True)\n",
    "    print(lam_fair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Split</th>\n",
       "      <th>Type</th>\n",
       "      <th>Lam_fair</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Unfairness</th>\n",
       "      <th>Ploss</th>\n",
       "      <th>Relaxation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.681435</td>\n",
       "      <td>0.093638</td>\n",
       "      <td>0.609795</td>\n",
       "      <td>0.247244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.106208</td>\n",
       "      <td>0.622662</td>\n",
       "      <td>0.311905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.682777</td>\n",
       "      <td>0.090952</td>\n",
       "      <td>0.609888</td>\n",
       "      <td>0.236076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.103277</td>\n",
       "      <td>0.622148</td>\n",
       "      <td>0.298329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.683353</td>\n",
       "      <td>0.088270</td>\n",
       "      <td>0.610159</td>\n",
       "      <td>0.225333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.100356</td>\n",
       "      <td>0.621831</td>\n",
       "      <td>0.285250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.683161</td>\n",
       "      <td>0.085605</td>\n",
       "      <td>0.610593</td>\n",
       "      <td>0.215022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.097455</td>\n",
       "      <td>0.621699</td>\n",
       "      <td>0.272680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.682010</td>\n",
       "      <td>0.082956</td>\n",
       "      <td>0.611180</td>\n",
       "      <td>0.205107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.094573</td>\n",
       "      <td>0.621739</td>\n",
       "      <td>0.260582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.680859</td>\n",
       "      <td>0.080322</td>\n",
       "      <td>0.611908</td>\n",
       "      <td>0.195552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.091706</td>\n",
       "      <td>0.621940</td>\n",
       "      <td>0.248913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.680476</td>\n",
       "      <td>0.077701</td>\n",
       "      <td>0.612769</td>\n",
       "      <td>0.186330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.088853</td>\n",
       "      <td>0.622292</td>\n",
       "      <td>0.237642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.678941</td>\n",
       "      <td>0.075094</td>\n",
       "      <td>0.613755</td>\n",
       "      <td>0.177415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.086013</td>\n",
       "      <td>0.622787</td>\n",
       "      <td>0.226740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.677982</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.614859</td>\n",
       "      <td>0.168782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.083183</td>\n",
       "      <td>0.623416</td>\n",
       "      <td>0.216179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.676640</td>\n",
       "      <td>0.069918</td>\n",
       "      <td>0.616074</td>\n",
       "      <td>0.160409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.080363</td>\n",
       "      <td>0.624173</td>\n",
       "      <td>0.205933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.674722</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.144332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.074733</td>\n",
       "      <td>0.626050</td>\n",
       "      <td>0.186255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.673571</td>\n",
       "      <td>0.062237</td>\n",
       "      <td>0.620335</td>\n",
       "      <td>0.136663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.071949</td>\n",
       "      <td>0.627150</td>\n",
       "      <td>0.176870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.672804</td>\n",
       "      <td>0.059697</td>\n",
       "      <td>0.621948</td>\n",
       "      <td>0.129148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>0.069155</td>\n",
       "      <td>0.628362</td>\n",
       "      <td>0.167672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.671078</td>\n",
       "      <td>0.057165</td>\n",
       "      <td>0.623650</td>\n",
       "      <td>0.121808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.066366</td>\n",
       "      <td>0.629677</td>\n",
       "      <td>0.158691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.669735</td>\n",
       "      <td>0.054642</td>\n",
       "      <td>0.625440</td>\n",
       "      <td>0.114630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.063581</td>\n",
       "      <td>0.631091</td>\n",
       "      <td>0.149910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.668585</td>\n",
       "      <td>0.052127</td>\n",
       "      <td>0.627315</td>\n",
       "      <td>0.107601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.060799</td>\n",
       "      <td>0.632602</td>\n",
       "      <td>0.141315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.665708</td>\n",
       "      <td>0.049619</td>\n",
       "      <td>0.629273</td>\n",
       "      <td>0.100710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.648000</td>\n",
       "      <td>0.058019</td>\n",
       "      <td>0.634208</td>\n",
       "      <td>0.132891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.663598</td>\n",
       "      <td>0.047117</td>\n",
       "      <td>0.631313</td>\n",
       "      <td>0.093945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>0.055241</td>\n",
       "      <td>0.635906</td>\n",
       "      <td>0.124627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.661296</td>\n",
       "      <td>0.044622</td>\n",
       "      <td>0.633432</td>\n",
       "      <td>0.087297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.052465</td>\n",
       "      <td>0.637695</td>\n",
       "      <td>0.116509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.659379</td>\n",
       "      <td>0.042132</td>\n",
       "      <td>0.635632</td>\n",
       "      <td>0.080756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.049691</td>\n",
       "      <td>0.639573</td>\n",
       "      <td>0.108526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.654392</td>\n",
       "      <td>0.039648</td>\n",
       "      <td>0.637909</td>\n",
       "      <td>0.074315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.633000</td>\n",
       "      <td>0.046917</td>\n",
       "      <td>0.641538</td>\n",
       "      <td>0.100669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.649981</td>\n",
       "      <td>0.037168</td>\n",
       "      <td>0.640266</td>\n",
       "      <td>0.067960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.632000</td>\n",
       "      <td>0.044144</td>\n",
       "      <td>0.643592</td>\n",
       "      <td>0.092923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.647104</td>\n",
       "      <td>0.034691</td>\n",
       "      <td>0.642702</td>\n",
       "      <td>0.061685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.041370</td>\n",
       "      <td>0.645733</td>\n",
       "      <td>0.085280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.641350</td>\n",
       "      <td>0.032219</td>\n",
       "      <td>0.645216</td>\n",
       "      <td>0.055482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.038597</td>\n",
       "      <td>0.647961</td>\n",
       "      <td>0.077731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.639241</td>\n",
       "      <td>0.029749</td>\n",
       "      <td>0.647809</td>\n",
       "      <td>0.049343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.035823</td>\n",
       "      <td>0.650276</td>\n",
       "      <td>0.070266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.635788</td>\n",
       "      <td>0.027282</td>\n",
       "      <td>0.650483</td>\n",
       "      <td>0.043261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.033048</td>\n",
       "      <td>0.652678</td>\n",
       "      <td>0.062876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.630802</td>\n",
       "      <td>0.024817</td>\n",
       "      <td>0.653238</td>\n",
       "      <td>0.037228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>0.055552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.625432</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.656076</td>\n",
       "      <td>0.031236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.027496</td>\n",
       "      <td>0.657750</td>\n",
       "      <td>0.048285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.618911</td>\n",
       "      <td>0.019891</td>\n",
       "      <td>0.658998</td>\n",
       "      <td>0.025279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>0.024718</td>\n",
       "      <td>0.660422</td>\n",
       "      <td>0.041067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Split   Type  Lam_fair  Accuracy  Unfairness     Ploss  Relaxation\n",
       "0       0  Train      0.00  0.681435    0.093638  0.609795    0.247244\n",
       "1       0   Test      0.00  0.662000    0.106208  0.622662    0.311905\n",
       "2       0  Train      0.01  0.682777    0.090952  0.609888    0.236076\n",
       "3       0   Test      0.01  0.661000    0.103277  0.622148    0.298329\n",
       "4       0  Train      0.02  0.683353    0.088270  0.610159    0.225333\n",
       "5       0   Test      0.02  0.663000    0.100356  0.621831    0.285250\n",
       "6       0  Train      0.03  0.683161    0.085605  0.610593    0.215022\n",
       "7       0   Test      0.03  0.666000    0.097455  0.621699    0.272680\n",
       "8       0  Train      0.04  0.682010    0.082956  0.611180    0.205107\n",
       "9       0   Test      0.04  0.667000    0.094573  0.621739    0.260582\n",
       "10      0  Train      0.05  0.680859    0.080322  0.611908    0.195552\n",
       "11      0   Test      0.05  0.669000    0.091706  0.621940    0.248913\n",
       "12      0  Train      0.06  0.680476    0.077701  0.612769    0.186330\n",
       "13      0   Test      0.06  0.668000    0.088853  0.622292    0.237642\n",
       "14      0  Train      0.07  0.678941    0.075094  0.613755    0.177415\n",
       "15      0   Test      0.07  0.665000    0.086013  0.622787    0.226740\n",
       "16      0  Train      0.08  0.677982    0.072500  0.614859    0.168782\n",
       "17      0   Test      0.08  0.666000    0.083183  0.623416    0.216179\n",
       "18      0  Train      0.09  0.676640    0.069918  0.616074    0.160409\n",
       "19      0   Test      0.09  0.665000    0.080363  0.624173    0.205933\n",
       "20      0  Train      0.11  0.674722    0.064775  0.618824    0.144332\n",
       "21      0   Test      0.11  0.659000    0.074733  0.626050    0.186255\n",
       "22      0  Train      0.12  0.673571    0.062237  0.620335    0.136663\n",
       "23      0   Test      0.12  0.658000    0.071949  0.627150    0.176870\n",
       "24      0  Train      0.13  0.672804    0.059697  0.621948    0.129148\n",
       "25      0   Test      0.13  0.657000    0.069155  0.628362    0.167672\n",
       "26      0  Train      0.14  0.671078    0.057165  0.623650    0.121808\n",
       "27      0   Test      0.14  0.659000    0.066366  0.629677    0.158691\n",
       "28      0  Train      0.15  0.669735    0.054642  0.625440    0.114630\n",
       "29      0   Test      0.15  0.655000    0.063581  0.631091    0.149910\n",
       "30      0  Train      0.16  0.668585    0.052127  0.627315    0.107601\n",
       "31      0   Test      0.16  0.650000    0.060799  0.632602    0.141315\n",
       "32      0  Train      0.17  0.665708    0.049619  0.629273    0.100710\n",
       "33      0   Test      0.17  0.648000    0.058019  0.634208    0.132891\n",
       "34      0  Train      0.18  0.663598    0.047117  0.631313    0.093945\n",
       "35      0   Test      0.18  0.641000    0.055241  0.635906    0.124627\n",
       "36      0  Train      0.19  0.661296    0.044622  0.633432    0.087297\n",
       "37      0   Test      0.19  0.642000    0.052465  0.637695    0.116509\n",
       "38      0  Train      0.20  0.659379    0.042132  0.635632    0.080756\n",
       "39      0   Test      0.20  0.636000    0.049691  0.639573    0.108526\n",
       "40      0  Train      0.21  0.654392    0.039648  0.637909    0.074315\n",
       "41      0   Test      0.21  0.633000    0.046917  0.641538    0.100669\n",
       "42      0  Train      0.22  0.649981    0.037168  0.640266    0.067960\n",
       "43      0   Test      0.22  0.632000    0.044144  0.643592    0.092923\n",
       "44      0  Train      0.23  0.647104    0.034691  0.642702    0.061685\n",
       "45      0   Test      0.23  0.629000    0.041370  0.645733    0.085280\n",
       "46      0  Train      0.24  0.641350    0.032219  0.645216    0.055482\n",
       "47      0   Test      0.24  0.624000    0.038597  0.647961    0.077731\n",
       "48      0  Train      0.25  0.639241    0.029749  0.647809    0.049343\n",
       "49      0   Test      0.25  0.620000    0.035823  0.650276    0.070266\n",
       "50      0  Train      0.26  0.635788    0.027282  0.650483    0.043261\n",
       "51      0   Test      0.26  0.617000    0.033048  0.652678    0.062876\n",
       "52      0  Train      0.27  0.630802    0.024817  0.653238    0.037228\n",
       "53      0   Test      0.27  0.610000    0.030273  0.655170    0.055552\n",
       "54      0  Train      0.28  0.625432    0.022354  0.656076    0.031236\n",
       "55      0   Test      0.28  0.604000    0.027496  0.657750    0.048285\n",
       "56      0  Train      0.29  0.618911    0.019891  0.658998    0.025279\n",
       "57      0   Test      0.29  0.597000    0.024718  0.660422    0.041067"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"./adult_dp_results.csv\")\n",
    "df_0 = df.loc[(df['Split']==0) & (df['Type']=='Test')].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0['Unfairness'] = np.abs(df_0['Unfairness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc882605a00>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_0 = df_0.sort_values(by='Accuracy')\n",
    "unfairness_vals = df_0['Unfairness'].values\n",
    "min_unfairness = [np.min(unfairness_vals[i:]) for i in range(len(unfairness_vals))]\n",
    "plt.plot(df_0['Accuracy'], min_unfairness)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"./adult_dp_results.csv\")\n",
    "df_0['min_unfairness_vals'] = min_unfairness\n",
    "df_0.to_csv(\"../compas_linear_eo_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
